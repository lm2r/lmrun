name: benched

resources:
  accelerators: L4:4
  use_spot: True
  disk_size: 128

envs:
  # Hugging Face repo id 'namespace/repo_name'
  MODEL:
  # model version: commit hash from model repository
  VERSION: 
  # vLLM serve CLI options passed in a single quoted string
  SERVE_OPTS: "-tp 4"
  # region where the main K3s server is hosted to retrieve cluster connection details
  AWS_DEFAULT_REGION: us-east-1

file_mounts:
  /r2:
    source: r2://lmrun

# proxy inference requests through a K8s service mapping vLLM port 8000 to a node port
setup: |
  set -e
  install -m 755 /r2/setup/{k3s_*,vllm-conda-install.sh} .
  sudo -E ./k3s_agent.py --app-port 8000 --node-port 30300
  ./vllm-conda-install.sh

# add '-r' to jq to exclude quotes from model name and fix "model name not found"
# max-model-len & gpu-memory-utilization optimized for wide model compatibility
run: |
  conda activate vllm
  model_name=`echo $SKYPILOT_CLUSTER_INFO | jq -r .cluster_name`
  vllm serve $SERVE_OPTS --generation-config auto \
      --max-model-len 31120 --gpu-memory-utilization 0.94 \
      --served-model-name $model_name --revision $VERSION $MODEL