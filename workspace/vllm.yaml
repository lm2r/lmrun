name: vLLM

resources:
  # big enough for on-disk model (GB)
  disk_size: 128
  # defined with --gpus in `sky launch`, to align other CLI parameters
  # accelerators:

envs:
  # Hugging Face repo id 'namespace/repo_name'
  MODEL:
  # model version: commit hash from model repository
  VERSION: 
  # vLLM serve CLI options passed in a single quoted string
  SERVE_OPTS: ""

file_mounts:
  # allow the local persistence of artefacts to R2 bucket
  # -> do not load weights from the local path: throttled throughput
  /r2:
    source: r2://lmrun

# create Python 3.12 (last compatible) environmnent to install vLLM
setup: |
  conda init bash
  conda create -n vllm python=3.12 -y
  conda activate vllm
  pip install vllm

# 'auto' generation config loads it from the model
run: |
  conda activate vllm
  vllm serve $SERVE_OPTS --gpu-memory-utilization 1 \
      --generation-config auto --revision $VERSION $MODEL