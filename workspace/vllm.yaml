name: vLLM

resources:
  # big enough for on-disk model (GB)
  disk_size: 128
  # defined with --gpus in `sky launch`, to align other CLI parameters
  # accelerators:

envs:
  # Hugging Face repo id 'namespace/repo_name'
  MODEL:
  # model version: commit hash from model repository
  VERSION: 
  # vLLM serve CLI options passed in a single quoted string
  SERVE_OPTS: ""

file_mounts:
  /r2:
    source: r2://lmrun

setup: |
  install -m 755 /r2/setup/vllm-conda-install.sh .
  ./vllm-conda-install.sh

# 'auto' generation config loads it from the model
run: |
  conda activate vllm
  vllm serve $SERVE_OPTS --gpu-memory-utilization 1 \
      --generation-config auto --revision $VERSION $MODEL